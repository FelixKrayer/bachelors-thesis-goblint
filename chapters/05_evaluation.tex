% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}
  In this chapter we evaluate our approach. This is split into two parts: First we test, whether our implementation is sound, i.e., it does not lead to wrong results. After that we benchmark the implementation on some real world C programs. The goal of the benchmark is to get a perspective on the precision and computation time of our proposed improvements from \autoref{chapter:precisionLossVariableAnalyses} and \autoref{chapter:precisionLossThreadAnalyses}. Thus, we compare context-insensitive runs with our improvement to context-insensitive ones without the improvement and to context-sensitive runs. In particular, we want to know how much of the precision lost by context insensitivity can be recovered through our proposed improvements.

  \section{Testing}
    The aim of this section is to ensure, that the addition of the \texttt{taintPartialContexts} analysis to \gob\ as described in \autoref{sec:implementation} does not lead to wrong results. Additionally, we want to make sure that just activating the taint analysis on a fully context-sensitive analysis run does not lead to less precise results.\\
    \\
    \gob\ provides an extensive set of regression test cases already. These test edge cases of various features of the analyzer. Each test case comes with a specific configuration, that should be used when executing the test.\\
    To verify our implementation, we ran all regression test cases with their specified configuration, but additionally activated the taint analysis. This helps to ensure that no precision is lost, just by activating the taint analysis. These test runs helped to find some bugs, which then were fixed.\\
    For the \texttt{threadCreate} analysis we used the same approach, where we ran all regression tests with the thread-create analysis additionally activated.\\
    We also contributed a few new regression tests which specifically test edge cases of the taint and the thread-create analysis. These include tests for each existing analysis, where we included either of our two new analyses, as well as tests for bugs we cleared to show the correct behavior after the fix. The new regression tests also aim to demonstrate how our changes improve the precision of existing analyses.\\
    \\
    Additionally, we investigate the results of the benchmarks we describe in the following section. These reinforce the verdict that the taint analysis and the thread-create analysis are sound in their implementation.
  
  \section{Benchmarking the improved \texttt{base} analysis}
    In this section we investigate, if and to what extent the changes we proposed in \autoref{sec:improveVariableAnalyses} provide an improvement. In particular, we compare analyses with different configurations in terms of precision and computation time. However, we focus on benchmarking the changes to the \texttt{base} analysis as this is the main variable analysis used in \gob. We do perform benchmarks with the other variable analyses we improved in \autoref{sec:improveVariableAnalyses} besides the \texttt{base} analysis.
    \subsection{SV-Benchmarks}\label{sec:benchSVbench}
      The first benchmarking approach we will describe in this section uses the SV-Benchmarks' "Collection of Verification Tasks" \parencite{svBench}. This collection of verification tasks is "constructed and maintained as a common benchmark for evaluating the effectiveness and efficiency of state-of-the-art verification technology" \parencite{svBench}. Each verification task consists of a program and a corresponding specification, i.e., a set of properties. The verifier to be benchmarked, i.e., in our case the \gob\ analyzer with our changes, then performs an analysis run on a given program. After that it is checked, whether the verifier was able to proof the given set of properties. The properties we focus on for our benchmarks are the following:
      \begin{itemize}
        \item \texttt{unreach-call}: A specified function \texttt{reach\_error()} in the program is never called during runtime.
        \item \texttt{no-overflow}: No integer overflow occurs in the program.
        \item \texttt{no-data-race}: The program contains no race condition.
      \end{itemize}
      For each of these properties, we perform three benchmark runs with different configurations on each program for which it is specified that the property holds.
      The three configurations we use for the benchmark runs are:
      \begin{itemize}
        \item \textit{sens}: The \texttt{base} analysis is performed context sensitively.
        \item \textit{insens}: The \texttt{base} analysis is performed context insensitively.
        \item \textit{insens taint}: The \texttt{base} analysis is performed context insensitively with the improvements of the taint analysis.
      \end{itemize}
      During each benchmark run with a certain configuration we save different kinds of information to compare later. The most important of which are the computation time per program and the total number of programs, for which the property to prove was proven.\\
      We first investigate the precision. For this we compare the number of programs for which the property was proven per configuration and property: The raw numbers can be found in \autoref{tbl:resultSVbench}, while the graph in \autoref{fig:resultSVbench} shows a visualization of the results.\\
 
      \begin{figure}[!ht]
        \centering
        \begin{tikzpicture}
          \begin{axis}[
              width=0.7\linewidth,
              ybar=0pt,
              ymin=0,ymax=100,
              xtick=data,
              symbolic x coords={\texttt{unreach-call}, \texttt{no-overflow}, \texttt{no-data-race}},
              enlarge x limits={abs=50pt},
              bar width = 15pt,
              ylabel= \% property proven, 
              ytick align=outside, 
              ytick pos=left,
              major x tick style = transparent,
              %x tick label style={rotate=45},
              legend style={at={(0.04,0.96)},anchor=north east, font=\footnotesize, legend cell align=left,},
              ]    
              \addplot[ybar,fill=TUMBlue, area legend] coordinates {
                (\texttt{unreach-call},18.61791015)
                (\texttt{no-overflow},40.59476044)
                (\texttt{no-data-race},74.41860465)};
              \addplot[ybar,fill=TUMAccentOrange, area legend] coordinates {
                (\texttt{unreach-call},18.81450899)
                (\texttt{no-overflow},40.51215483)
                (\texttt{no-data-race},74.41860465)}; 
              \addplot[ybar,fill=TUMAccentGreen, area legend] coordinates {
                (\texttt{unreach-call},18.92263836)
                (\texttt{no-overflow},40.78357328)
                (\texttt{no-data-race},74.30786268)}; 
          \legend{insens, insens taint, sens}  
          \end{axis}
        \end{tikzpicture}
        \caption{Bar graph for the comparison of the three configurations per property. A bar represents the percentage of programs, for which the corresponding property was proven using the corresponding configuration}
        \label{fig:resultSVbench}
      \end{figure}

      \begin{table}
        \centering
        \begin{tabular}{l|l||r|r||r|r|r}
          %\multicolumn{1}{l|}{property} & \multicolumn{1}{l||}{config} & \multicolumn{1}{l|}{\# proven} & \multicolumn{1}{l||}{\# total} & \multicolumn{1}{l|}{\% loss} & \multicolumn{1}{l|}{\% recovered} \\
          \textit{property} & \textit{config.} & \textit{\# proven} & \textit{\# total} & \textit{\# loss by} & \textit{\# recovered} & \textit{\% recovered}\\
            &  &  &  & \textit{insens} & & \textit{of loss}\\
          \hline
          \texttt{unreach-call} & \textit{sens} & 1925 & 10173 & 31 & 20 & 64.52\\
          & \textit{insens} & 1894 & & & & \\
          & \textit{insens taint} & 1914 & & & & \\
          \hline
          \texttt{no-overflow} & \textit{sens} & 3456 & 8474 & 16 & -17 & -43.75\\
          & \textit{insens} & 3440 & & & & \\
          & \textit{insens taint} & 3433 & & & & \\
          \hline
          \texttt{no-data-race} & \textit{sens} & 671 & 903 & -1 & 0 & 0.00\\
          & \textit{insens} & 672 & & & & \\
          & \textit{insens taint} & 672 & & & & \\
          \hline
        \end{tabular}
        \caption{Table showing the results of the SV-Benchmarks}
        \label{tbl:resultSVbench}
      \end{table}

     

      As we can see in \autoref{fig:resultSVbench}, there is in fact not a big difference between the three configurations for each property. Our interpretation of this result is, that the choice between our three configurations only has a small effect on the precision of \gob when analyzing real world programs. In other words this means, that not a lot of precision is lost by performing the \texttt{base} analysis context insensitively. The taint analysis can only recover lost precision, so little over all precision loss means, that the taint analysis can only provide little over all improvement.\\
      \\
      We now take a closer look at \autoref{tbl:resultSVbench}. First we talk about the \texttt{unreach-call} property: We see, that with the \textit{insens} configuration, the analyzer was able to proof the property 31 times fewer than with the \textit{sens} configuration. However, it proved the property 20 times more often with the \textit{insens taint} configuration compared to the \textit{insens} configuration. This leads us to the conclusion, that for this property the taint analysis helped to recover about 65\% of the precision lost.\\
      As for the results of the \texttt{no-overflow} property, it seems like the usage of the taint analysis leads to less precise results. This property was proven for 17 fewer programs, when using the \textit{insens taint} configuration instead of the \textit{insens} configuration. In fact these results helped us find a bug in our implementation. This bug is now fixed and the \texttt{no-overflow} property can now be proven for these 17 programs with the \textit{insens taint} configuration. In conclusion however, we cannot say that the taint analysis provided an improvement for proving the \texttt{no-overflow} property.\\
      Similarly, the \textit{insens taint} configuration is exactly as precise as the \texttt{insens} configuration for the \texttt{no-data-race} property. The reason, that the \textit{sens} configuration proved the property for one fewer program than both of the other configurations is a timeout. Considering this, we come to the conclusion, that for this property all three configurations are equally precise.\\
      \\
      <TODO: TIMINGS!!!>\\
      \begin{tabular}{l|l||r|r|r|r}
        \textit{property} & \textit{config.} & \textit{avg. cputime} & \textit{avg. cputime} & \textit{\# Timeouts} & \textit{\# Errors} \\
        & & \textit{all tasks (s)} & \textit{no errors (s)} & & \\
        \hline
        \texttt{unreach-call} & \textit{sens} & 191 & 30.8 & 1712 & 1934 \\
        & \textit{insens} & 188 & 28.1 & 1563 & 1799 \\
        & \textit{insens taint} & 188 & 31.2 & 1658 & 1908 \\
        \hline
        \texttt{no-overflow} & \textit{sens} & 7.51 & 0.722 & 51 & 130 \\
        & \textit{insens} & 5.25 & 0.627 & 42 & 43 \\
        & \textit{insens taint} & 5.37 & 0.694 & 44 & 45\\
        \hline
        \texttt{no-data-race} & \textit{sens} & 1.51 & 0.510 & 1 & 1\\
        & \textit{insens} & 0.517 & 0.517 & 0 & 0 \\
        & \textit{insens taint} & 0.525 & 0.525 & 0 & 0 \\
        \hline
      \end{tabular}

    \subsection{GNU Coreutils}\label{sec:benchCoreutils}
      We also benchmark the improved \texttt{base} analysis with another approach. For this we use modified versions of C programs from the GNU Core Utilities "coreutils" \parencite{gnuCoreutils}. These programs implement "the basic file, shell and text manipulation utilities of the GNU operating system. These are the core utilities which are expected to exist on every operating system."\parencite{gnuCoreutils}\\
      Combined versions of the coreutil programs are found in a benchmark repository dedicated for benchmarking the \gob\ analyzer \parencite{goblintBench}. A "combined version" of a program is a code file, where all dependencies of included files of the program are merged into one single code file.\\
      For these combined programs we use a feature from the \gob\ analyzer itself to generate assertions at different points within the program. An assertion is an equality or inequality involving program variables, that holds for every concrete execution of the program. To generate these, the analyzer performs an analysis with a given configuration on a given program. \gob\ then uses the information it gains to place assertions which it knows are true in the program and produces an output file.\\
      We can then use the resulting file with generated assertions to compare other analysis runs with different configurations. The metric for these comparisons is the number of proven assertions. It is necessary that the configuration which is used to generate the assertions is at least as precise as the most precise configuration of the runs we want to compare. Only the can this benchmarking approach to produce meaningful results.\\
      For this reason we generate assertions with a configuration that performs the \texttt{base} analysis context sensitively. In order to do this we followed an approach similar to Julian Erhard, where he generated asserts for theses programs context insensitively. A detailed description of his approach can be found in \parencite{svBenchCoreutils}.\\
      We compare the same configurations we compared in \autoref{sec:benchSVbench}.\\
      
      \begin{table}
        \centering
        \begin{tabular}{l|l||r|r||r|r|r}
          \textit{program} & \textit{config.} & \textit{\# asserts} & \textit{\# total} & \textit{\% loss by} & \textit{\% recovered} & \textit{\% recovered}\\
            &  & \textit{proven} & \textit{asserts} & \textit{insens} & \textit{of total} & \textit{of loss}\\
          \hline
          \texttt{cksum\_comb.c} & \textit{sens} & 1998 & 1998 & 1.30 & 0.00 & 0.00\\
          & \textit{insens} & 1972 & 1998 & & & \\
          & \textit{insens taint} & 1972 & 1998 & & & \\
          \hline
          \texttt{cut\_comb.c} & \textit{sens} & 3992 & 3992 & 0.30 & 0.00 & 0.00\\
          & \textit{insens} & 3979 & 3992 & & & \\
          & \textit{insens taint} & 3979 & 3992 & & & \\
          \hline
          \texttt{dd\_comb.c} & \textit{sens} & 4462 & 4464 & 2.97 & 0.49 & 16.54\\
          & \textit{insens} & 4337 & 4472 & & & \\
          & \textit{insens taint} & 4359 & 4472 & & & \\
          \hline          
          \texttt{df\_comb.c} & \textit{sens} & 8834 & 8834 & 12.68 & 0.05 & 0.36\\
          & \textit{insens} & 7714 & 8834 & & & \\
          & \textit{insens taint} & 7718 & 8834 & & & \\
          \hline         
          \texttt{du\_comb.c} & \textit{sens} & 9810 & 9810 & 3.41 & 0.05 & 1.50\\
          & \textit{insens} & 9464 & 9798 & & & \\
          & \textit{insens taint} & 9469 & 9798 & & & \\
          \hline         
          \texttt{nohup\_comb.c} & \textit{sens} & 3397 & 3397 & 0.77 & 0.00 & 0.00\\
          & \textit{insens} & 3371 & 3397 & & & \\
          & \textit{insens taint} & 3371 & 3397 & & & \\
          \hline         
          \texttt{ptx\_comb.c} & \textit{sens} & 5786 & 5786 & 4.46 & 0.07 & 1.55\\
          & \textit{insens} & 5528 & 5786 & & & \\
          & \textit{insens taint} & 5532 & 5786 & & & \\
          \hline         
          \texttt{tail\_comb.c} & \textit{sens} & 4806 & 4806 & 0.50 & 0.00 & 0.00\\
          & \textit{insens} & 4782 & 4806 & & & \\
          & \textit{insens taint} & 4782 & 4806 & & & \\
          \hline
        \end{tabular}
        \caption{Table showing the results of the coreutils with generated assertions}
        \label{tbl:resultCoreutils}
      \end{table}

      The results of the benchmark runs on the coreutil programs can be seen in \autoref{tbl:resultCoreutils}. We omit the results for two further programs \texttt{cp\_comb.c} and \texttt{mv\_comb.c} because for these the analyzer proved some assertions to evaluate to "false" with the \textit{insens} and \textit{insens taint}. This hints at bugs in the analyzer which are unrelated to our changes.\\
      The numbers of total assertions differs between the \textit{sens} and both \textit{insens (taint)} configurations for some programs. The reason for this is, that an assertion in a function can be checked multiple times, once for each context the function is evaluated with. Therefore, we compare percentages for this benchmark.\\
      We make the following observations: The percentage of the precision loss that is recovered reaches over 16\% for one program while it stays below 2\% for all other programs. For three of the eight programs shown, no improvement was achieved at all. From this we conclude that it depends a lot on the program which analyzed, how much the taint analysis can improve the precision loss.\\
      We also note that the "\textit{\% recovered of total}" is rather low. That is the percentage of assertions, which were found with the \textit{insens taint} but not the \textit{insens} configuration, never surpasses 0.5\%. Therefore, we conclude that the effect of our changes on the overall precision of the analyzer is very limited.      
    
  \section{Benchmarking the improved \texttt{threadId} analysis}
    We only performed a partial benchmark for our improvement of the \texttt{threadId} analysis using the thread-create analysis. For this we again used the GNU coreutil programs, which we described in \autoref{sec:benchCoreutils}. This time however, instead of comparing the number of correctly proven assertions, we compare the number of race conditions, the \gob\ analyzer finds in each program with each configuration. We compared these configurations:
    \begin{itemize}
      \item \textit{sens}: The \texttt{threadId} analysis is performed context sensitively.
      \item \textit{part-sens}: The \texttt{threadId} analysis is performed partially context sensitively, i.e., only sensitive with respect to the thread ID but not with respect to the set of encountered create edges. Concretely, the option \texttt{ana.thread.context.createEdges} we added is disabled in this configuration.
      \item \textit{part-sens taint}: Same configuration like \textit{part-sens} but with the improvements of the thread-create analysis enabled.
    \end{itemize}
    The results of this benchmark are as follows: For each of the coreutil programs, the analyzer finds between 5 and 8 race conditions. However, there is no difference between the three configurations. This means that for each program, the number of race conditions found is the same irregardless of the configuration chosen. This means that with this benchmarking setup we do not record any precision loss that results from partial context-sensitivity. Since no precision is lost, we cannot see, if the thread-create analysis provides an improvement.
    \\
    In conclusion there definitely exist cases, where the running the \texttt{threadId} partially context-sensitive leads to precision loss, which the thread-create analysis can reduce. We have shown this by adding a regression test of one such case to the \gob\ regression tests, similar to the example in \autoref{fig:example_thread}.\\
    However, such cases may only exist very rarely in real world programs.\\
    It has to be noted that our benchmark for this analysis was only very small in scale. More extensive benchmarks are necessary to make a more meaningful statement. The next step in the future is to perform a benchmark run of the SV-Benchmarks' \texttt{unreach-call} property as described in \autoref{sec:benchSVbench}.
