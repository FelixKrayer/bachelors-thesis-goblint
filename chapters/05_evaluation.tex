% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}
  In this chapter we evaluate our approach. This is split into two parts: First we test, whether our implementation is sound, i.e., it does not lead to wrong results. After that we benchmark the implementation on some real world C programs. The goal of the benchmark is to get a perspective on the precision and computation time of our proposed improvements from \autoref{chapter:precisionLossVariableAnalyses} and \autoref{chapter:precisionLossThreadAnalyses}. Thus, we compare context-insensitive runs with our improvement to context-insensitive ones without the improvement and to context-sensitive runs. In particular, we want to know how much of the precision lost by context insensitivity can be recovered through our proposed improvements.

  \section{Testing}
    The goal of this section is to ensure, that the addition of the \texttt{taintPartialContexts} analysis to \gob\ as described in \autoref{sec:implementation} does not lead to wrong results. Additionally, we want to make sure that just activating the taint analysis on a fully context-sensitive analysis run does not lead to less precise results.\\
    \\
    \gob\ provides an extensive set of regression test cases already. These test edge cases of various features of the analyzer. Each test case comes with a specific configuration, that should be used when executing the test.\\
    To verify our implementation, we ran all regression test cases with their specified configuration, but additionally activated the taint analysis. This helps to ensure that no precision is lost, just by activating the taint analysis. These test runs helped to find some bugs, which then were fixed.\\
    For the \texttt{threadCreate} analysis we used the same approach, where we ran all regression tests with the thread-create analysis additionally activated.\\
    We also contributed a few new regression tests which specifically test edge cases of the taint and the thread-create analysis. These include tests for each existing analysis, where we included our two new analyses, as well as tests for bugs we cleared to show the correct behavior after the fix. The new regression tests also aim to demonstrate how our changes improve the precision of existing analyses.\\
    \\
    Additionally, we investigate the results of the benchmarks we describe in the following section. These reinforce the verdict that the taint analysis and the thread create analysis are sound in their implementation.
  
  \section{Benchmarking}
    %TODO: sentence
    \subsection{Benchmarks for improved \texttt{base} analysis}
      % TODO: Specify, that it is for Variable analysis and that we only benchmark "base"
      For benchmarking the improved \texttt{base} analysis (see \autoref{sec:improveVariableAnalyses}) we use the SV-Benchmarks' "Collection of Verification Tasks" \parencite{svBench}. This collection of verification tasks is "constructed and maintained as a common benchmark for evaluating the effectiveness and efficiency of state-of-the-art verification technology" \parencite{svBench}. Each verification task consists of a program and a corresponding specification, i.e., a set of properties. The verifier to be benchmarked, i.e., in our case the \gob\ analyzer with our changes, then performs an analysis run on a given program. After that it is checked, whether the verifier was able to proof the given set of properties. The properties we focus on for our benchmarks are the following:
      \begin{itemize}
        \item \texttt{unreach-call}: A specified function \texttt{reach\_error()} in the program is never called during runtime.
        \item \texttt{no-overflow}: No integer overflow occurs in the program.
        \item \texttt{no-data-race}: The program contains no race condition.
      \end{itemize}
      For each of these properties, we perform three benchmark runs with different configurations on each program for which it is specified that the property holds.
      The three configurations we use for the benchmark runs are:
      \begin{itemize}
        \item The \texttt{base} analysis is performed context-sensitively.
        \item The \texttt{base} analysis is performed context-insensitively.
        \item The \texttt{base} analysis is performed context-insensitively with the improvements of the taint analysis.
      \end{itemize}
      During each benchmark run with a certain configuration we save different kinds of information to compare later. The most important of which are the computation time per program and the total number of programs, for which the property to prove was proven.\\
      We then compare the number of programs for which the property was proven per configuration and property. We visualize the results with the following graph:

      \begin{figure}
        \centering
        \begin{tikzpicture}
          \begin{axis}[
              width=0.7\linewidth,
              ybar=0pt,
              ymin=0,ymax=100,
              xtick=data,
              symbolic x coords={\texttt{unreach-call}, \texttt{no-overflow}, \texttt{no-data-race}},
              enlarge x limits={abs=50pt},
              bar width = 15pt,
              ylabel= \% property proven, 
              ytick align=outside, 
              ytick pos=left,
              major x tick style = transparent,
              %x tick label style={rotate=45},
              legend style={at={(0.04,0.96)},anchor=north east, font=\footnotesize, legend cell align=left,},
              ]    
              \addplot[ybar,fill=TUMBlue, area legend] coordinates {
                (\texttt{unreach-call},18.61791015)
                (\texttt{no-overflow},40.59476044)
                (\texttt{no-data-race},74.41860465)};
              \addplot[ybar,fill=TUMAccentOrange, area legend] coordinates {
                (\texttt{unreach-call},18.81450899)
                (\texttt{no-overflow},40.51215483)
                (\texttt{no-data-race},74.41860465)}; 
              \addplot[ybar,fill=TUMAccentGreen, area legend] coordinates {
                (\texttt{unreach-call},18.92263836)
                (\texttt{no-overflow},40.78357328)
                (\texttt{no-data-race},74.30786268)}; 
          \legend{insens, insens taint, sens}  
          \end{axis}
        \end{tikzpicture}
      \end{figure}

      We also benchmark the improved \texttt{base} analysis with another approach. For this we use modified versions of C programs from the GNU Core Utilities "coreutils" \parencite{gnuCoreutils}. These programs implement "the basic file, shell and text manipulation utilities of the GNU operating system. These are the core utilities which are expected to exist on every operating system."\parencite{gnuCoreutils}\\
      Combined versions of the coreutil programs are found in a benchmark repository dedicated for benchmarking the \gob\ analyzer \parencite{goblintBench}. A "combined version" of a program is a code file, where all dependencies of included files of the program are merged into one single code file.\\
      For these combined programs we use a feature from the \gob\ analyzer itself to generate assertions at different points within the program. An assertion is an equality or inequality involving program variables, that holds for every concrete execution of the program. To generate these, the analyzer performs an analysis with a given configuration on a given program. \gob\ then uses the information it gains to place assertions which it knows are true in the program and produces an output file.\\
      We can then use the resulting file with generated assertions to compare other analysis runs with different configurations. The metric for these comparisons is the number of proven assertions. It is necessary that the configuration which is used to generate the assertions is at least as precise as the most precise configuration of the runs we want to compare. Only the can this benchmarking approach to produce meaningful results.\\
      For this reason we generate assertions with a configuration that performs the \texttt{base} analysis context sensitively.\\
    
    \subsection{Benchmarks for improved \texttt{threadId} analysis}

